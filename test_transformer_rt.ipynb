{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b833219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from dataset import LinearDynamicalDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from model_ts import GPTConfig, GPT\n",
    "import tqdm\n",
    "import argparse\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c25428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall settings\n",
    "out_dir = \"out\"\n",
    "\n",
    "# System settings\n",
    "nx = 10\n",
    "nu = 1\n",
    "ny = 1\n",
    "seq_len = 400\n",
    "\n",
    "\n",
    "# Compute settings\n",
    "cuda_device = \"cuda:0\"\n",
    "no_cuda = False\n",
    "threads = 20\n",
    "compile = True\n",
    "batch_size = 256\n",
    "\n",
    "# Create out dir\n",
    "out_dir = Path(out_dir)\n",
    "exp_data = torch.load(out_dir/\"ckpt.pt\")\n",
    "\n",
    "# Configure compute\n",
    "torch.set_num_threads(threads)\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device_name  = cuda_device if use_cuda else \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "device_type = 'cuda' if 'cuda' in device_name else 'cpu' # for later use in torch.autocast\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "#torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "#torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "\n",
    "# Create data loader\n",
    "test_ds = LinearDynamicalDataset(nx=nx, nu=nu, ny=ny, seq_len=seq_len)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, num_workers=threads)\n",
    "\n",
    "model_args = exp_data[\"model_args\"]\n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf)\n",
    "state_dict = exp_data[\"model\"]\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict);\n",
    "model = model.to(device)\n",
    "#if compile:\n",
    "#    model = torch.compile(model)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cad475",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_y, batch_u = next(iter(test_dl))\n",
    "batch_y = batch_y.to(device)\n",
    "batch_u = batch_u.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56aae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call model like in training (future inputs/outputs known)\n",
    "with torch.no_grad():\n",
    "    batch_y_pred, _ = model(batch_u, batch_y, compute_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call model causally to see if it really behaves correctly\n",
    "batch_y_pred_rt = []\n",
    "seq_len = batch_y.shape[1]\n",
    "with torch.no_grad():\n",
    "    for idx in range(seq_len):\n",
    "        batch_y_t, _ = model(batch_u[:, :idx+1, :], batch_y[:, :idx+1, :], compute_loss=False)\n",
    "        batch_y_pred_rt.append(batch_y_t)\n",
    "batch_y_pred_rt = torch.cat(batch_y_pred_rt, dim=1)\n",
    "#batch_y_pred_rt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7277086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.testing.assert_close(batch_y_pred, batch_y_pred_rt)\n",
    "torch.max(torch.abs(batch_y_pred - batch_y_pred_rt)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call model in simulation from a certain time step!\n",
    "sim_start = 100\n",
    "batch_y_sim = torch.zeros_like(batch_y)\n",
    "batch_y_sim[:, :sim_start, :] = batch_y[:, :sim_start, :]\n",
    "with torch.no_grad():\n",
    "    for idx in range(sim_start, seq_len):\n",
    "        batch_y_t, _ = model(batch_u[:, :idx, :], batch_y_sim[:, :idx, :], compute_loss=False)\n",
    "        batch_y_sim[:, [idx], :] = batch_y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb80d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_y_pred = batch_y_pred.to(\"cpu\").detach().numpy()\n",
    "batch_y_pred_rt = batch_y_pred_rt.to(\"cpu\").detach().numpy()\n",
    "batch_y_sim = batch_y_sim.detach().to(\"cpu\").numpy()\n",
    "batch_y = batch_y.detach().to(\"cpu\").numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503e145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_y_target = batch_y[:, 1:, :] # target @ time k: y_{k+1}\n",
    "batch_y_pred = batch_y_pred[:, :-1, :] # prediction @ time k: y_{k+1|k}\n",
    "batch_y_sim = batch_y_sim[:, 1:, :] # simulation @ time k: y_{k+1|k}\n",
    "batch_pred_err = batch_y_target - batch_y_pred\n",
    "batch_sim_err = batch_y_target - batch_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9206d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = 1\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=batch_y_target[instance].squeeze(), name=\"y\", line_color=\"black\"))\n",
    "fig.add_trace(go.Scatter(y=batch_y_sim[instance].squeeze(), name=\"y_sim\", line_color=\"blue\"))\n",
    "#fig.add_trace(go.Scatter(y=batch_y_pred[instance].squeeze(), name=\"y_pred\", line_color=\"magenta\"))\n",
    "fig.add_vline(x=sim_start, line_color=\"red\", name=\"sim_start\")\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.plot(batch_y[1], 'k', label=\"True\")\n",
    "#plt.plot(batch_y_pred[0], 'b', label=\"Pred\")\n",
    "#plt.plot(batch_y_pred_rt[0], 'm', label=\"Pred\")\n",
    "#plt.plot(batch_y_sim[1], 'b', label=\"Sim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d95a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchid import metrics\n",
    "skip = sim_start\n",
    "rmse_pred = metrics.rmse(batch_y_target[:, skip:, :], batch_y_pred[:, skip:, :], time_axis=1)\n",
    "rmse_sim = metrics.rmse(batch_y_target[:, skip:, :], batch_y_sim[:, skip:, :], time_axis=1)\n",
    "#rmse_z = metrics.rmse(batch_y_target[:, skip:, :], 0*batch_y_sim[:, skip:, :], time_axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4bddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_pred.mean(), rmse_sim.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd8daad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"RMSE\")\n",
    "plt.hist(rmse_sim, color=\"black\", label=\"sim\");\n",
    "plt.hist(rmse_pred, color=\"red\", label=\"pred\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6e8b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.boxplot([rmse_pred.ravel(), rmse_sim.ravel()], labels=[\"pred\", \"sim\"]);\n",
    "\n",
    "#plt.boxplot(rmse_pred);\n",
    "#ax.set_xticklabels(\"pred\")\n",
    "#plt.boxplot(rmse_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
